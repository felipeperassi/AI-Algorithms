{"cells":[{"cell_type":"markdown","id":"92e549b3","metadata":{"id":"92e549b3"},"source":["<div style=\"text-align: center;\">\n","  <h1>TA136 - Taller de Procesamiento de Señales</h1>\n","  <h2>Trabajo Práctico 9: Modelo de Lenguaje y Sistema de Recomendación</h2>\n","</div>"]},{"cell_type":"markdown","id":"0f09ae69","metadata":{"id":"0f09ae69"},"source":["---\n","---"]},{"cell_type":"markdown","id":"228da0be","metadata":{"id":"228da0be"},"source":["<div style=\"text-align: center;\">\n","  <h3> Introducción\n","</div>"]},{"cell_type":"markdown","id":"baxw-G8i7XlO","metadata":{"id":"baxw-G8i7XlO"},"source":["&ensp; El presente trabajo práctico tiene como objetivo el diseño de un sistema de recomendación de películas, complementado con un modelo de lenguaje que permita buscar títulos aún si son ingresados con errores. El desarrollo se estructura en dos incisos principales: el buscador de películas y el sistema de recomendación.\n","\n","&ensp; En la primera etapa, se construye un modelo de lenguaje utilizando representaciones pre-entrenadas con $\\texttt{GloVe}$ y dimensión $300$. A partir de este modelo, se implementan funciones que permiten obtener vectores de palabras, formar representaciones de oraciones mediante una bolsa de palabras y calcular similitudes entre representaciones usando la similitud coseno. Esto, permite implementar un buscador capaz de sugerir títulos cercanos a lo que el usuario ingresa, incluso ante errores o imprecisiones.\n","\n","&ensp; Por otro lado, en la segunda etapa, se desarrolla un sistema de recomendación utilizando un filtro colaborativo. Para ello, se agrega un nuevo usuario con más de diez películas calificadas, y se entrena un modelo con *gradient descent*, usando un espacio latente de dimensión $10$ y regularización. Finalmente, se combinan los resultados del filtro colaborativo con la calificación promedio de las películas para obtener un *rating*, con el que se realiza una recomendación de cinco películas no vistas."]},{"cell_type":"markdown","id":"9f2109f1","metadata":{"id":"9f2109f1"},"source":["---\n","---"]},{"cell_type":"markdown","id":"16b29acd","metadata":{"id":"16b29acd"},"source":["<div style=\"text-align: center;\">\n","  <h3> Desarrollo\n","</div>"]},{"cell_type":"markdown","id":"cd9219f6","metadata":{"id":"cd9219f6"},"source":["**Se desea crear un sistema para recomendar películas. El archivo $\\texttt{movies.csv}$ posee una base de datos donde usuarios calificaron (del $1$ al $5$) diferentes películas ($0$ significa sin calificar).**"]},{"cell_type":"markdown","id":"b02d078a","metadata":{"id":"b02d078a"},"source":["---"]},{"cell_type":"markdown","id":"a37c881b","metadata":{"id":"a37c881b"},"source":["#### (A) *Modelo de Lenguaje:*"]},{"cell_type":"markdown","id":"8eb9eac6","metadata":{"id":"8eb9eac6"},"source":["**Se desea diseñar un buscador de títulos de películas, de manera que si el usuario comete algún error u omisión cuando lo escribe, el buscador pueda entender. Para ello, se estudiará la similitud entre los *embeddings*. A continuación se describen los pasos para diseñar el buscador; se recomienda que los mismos sean métodos dentro de la clase del buscador mencionado.**\n","\n","- **Descargar las representaciones pre-entrenadas $\\texttt{GloVe}$ de dimensión $300$.**\n","\n","- **Cargar el modelo de lenguaje.**\n","\n","- **Implementar un *word2vec*. Si la palabra está en el vocabulario debe devolver el vector del modelo de lenguaje, caso contrario debe devolver un vector de ceros.**\n","\n","- **Implementar una bolsa de palabras que transforme cualquier *string* en un vector. Los pasos a seguir son:**\n","    \n","    - **Convertir las mayúsculas en minúsculas.**\n","    - **Eliminar caracteres extraños.**\n","    - **Unificar espacios en blanco.**\n","    - **Convertir el *string* en una lista de palabras.**\n","    - **Convertir cada palabra en un *embedding* usando el *word2vec*.**\n","    - **Sumar las representaciones para formar un solo vector.**\n","\n","- **Se desea medir que tan parecidos son dos *embeddings*. Para ello, implementar un código que calcule la similitud coseno.**\n","\n","- **Implementar un buscador que, dado un *string* (y su correspondiente *embedding*), devuelva la película con una representación más similar.**\n","\n","&ensp; A fin de desarrollar el buscador de títulos de películas, se siguieron los lineamientos dados por la cátedra y se definió la clase llamada $\\texttt{LanguageModel}$. Esta implementa los métodos que se detallan a continuación:\n","\n","- `__init__:` Inicializa la clase y declara los atributos necesarios para almacenar los parámetros del modelo, tales como la dirección *url* y el nombre del archivo que contiene las representaciones preentrenadas de palabras con $\\texttt{GloVe}$, un diccionario para almacenar los *embeddings* de cada palabra y una lista con los *embeddings* correspondientes a los títulos de las películas. Durante esta inicialización, se ejecutan métodos internos del modelo: $\\texttt{\\_\\_download\\_model}$, $\\texttt{\\_\\_load\\_model}$ y $\\texttt{\\_\\_calculate\\_embedding\\_titles()}$; los cuales se explican a continuación.\n","\n","- `__download_model` y `__load_model:` Descarga las representaciones de $\\texttt{GloVe}$ con dimensión $300$ y las carga al diccionario inicializado anteriormente.\n","\n","- `word2vec:` Retorna el vector asociado a una palabra dada. Si la palabra no se encuentra en el modelo, se devuelve un vector nulo de dimensión $300$.\n","  \n","- `BoW:` Devuelve el *embedding* correspondiente a una *string* pasada por parámetro. Para ello, se siguen los pasos:\n","  1. Convertir el texto a minúsculas.\n","  2. Eliminar los caractéres que no son letras o espacios.\n","  3. Separar las palabras que componen a la *string*.\n","  4. Obtener el *embedding* correspondiente a cada palabra mediante $\\texttt{word2vec}$.\n","  5. Sumar los vectores de cada palabra, para así obtener el *embedding* de la cadena.\n","\n","- `sim_cos:` Calcula la similitud coseno entre dos *embeddings*. Para evitar divisiones por cero, si el denominador es nulo, se reemplaza por un valor mínimo $\\epsilon = 10^{-8}$.\n","\n","$$SC(u, ~v) = \\frac{u \\cdot v}{\\texttt{max(}||u|| \\cdot ||v||, ~ \\epsilon\\texttt{)}}$$\n","\n","- `__calculate_embedding_titles:` Genera y almacena los *embeddings* de todos los títulos de la base de datos a partir de la bolsa de palabras implementada.\n","\n","- `searcher:` Dado un título ingresado por el usuario, calcula su *embedding* y lo compara con los títulos de la base de datos mediante la similitud coseno. Luego, devuelve el título con mayor *SC*.\n"]},{"cell_type":"code","execution_count":null,"id":"c79d53ac","metadata":{"id":"c79d53ac"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","import re"]},{"cell_type":"code","execution_count":null,"id":"9d69bef9","metadata":{"id":"9d69bef9"},"outputs":[],"source":["class LanguageModel:\n","    def __init__(self, titles):\n","        self.titles = titles\n","        self.url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n","        self.filename = \"glove.6B.300d.txt\"\n","        self.language_model = {}\n","        self.embedding_titles = []\n","        self.__download_model()\n","        self.__load_model()\n","        self.__calculate_embedding_titles()\n","\n","    def __download_model(self):\n","        if not os.path.exists(self.filename):\n","            print(\"Descargando GloVe...\")\n","            os.system(f\"wget {self.url}\")\n","            os.system(\"unzip glove.6B.zip\")\n","\n","    def __load_model(self):\n","        with open(self.filename, encoding=\"utf-8\") as f:\n","            for line in f:\n","                parts = line.strip().split()\n","                word = parts[0]\n","                vec = np.array(parts[1:], dtype=float)\n","                self.language_model[word] = vec\n","\n","    def word2vec(self, word):\n","        return self.language_model.get(word, np.zeros(300))\n","\n","    def BoW(self, string):\n","        low_string = string.lower()\n","        words = re.sub(r\"[^a-z0-9 ]\", \"\", low_string).split()\n","        embeddings = [self.word2vec(word) for word in words]\n","        return sum(embeddings)\n","\n","    def sim_cos(self, embedding1, embedding2):\n","        return np.dot(embedding1, embedding2) / np.max([np.linalg.norm(embedding1) * np.linalg.norm(embedding2), 1e-8])\n","\n","    def __calculate_embedding_titles(self):\n","        for title in self.titles:\n","            self.embedding_titles.append(self.BoW(title))\n","\n","    def searcher(self, string):\n","        embedding = self.BoW(string)\n","        scores = [self.sim_cos(embedding, self.embedding_titles[i]) for i in range(len(self.embedding_titles))]\n","        return self.titles[np.argmax(scores)]\n"]},{"cell_type":"markdown","id":"Or8UejJ3eteF","metadata":{"id":"Or8UejJ3eteF"},"source":["&ensp; Una vez implementada la clase, se obtiene el dataset del github de la materia y se lo baja como un *dataframe* de la librería `pandas`."]},{"cell_type":"code","execution_count":null,"id":"Fc6v-PKwBkru","metadata":{"id":"Fc6v-PKwBkru"},"outputs":[],"source":["path = \"https://raw.githubusercontent.com/mvera1412/TA136-TB056-TB057-8625/main/data/movies.csv\"\n","DF = pd.read_csv(path)\n","titulos = DF.Name.tolist()"]},{"cell_type":"markdown","id":"lOMdfQPtfy_N","metadata":{"id":"lOMdfQPtfy_N"},"source":["&ensp; Luego, se inicializa la clase con los títulos extraídos del *dataframe* y se verifica el correcto funcionamiento del método correspondiente al buscador."]},{"cell_type":"code","execution_count":null,"id":"d4875786","metadata":{"id":"d4875786"},"outputs":[],"source":["LM = LanguageModel(titles=titulos)\n","LM.searcher(\"chocolate\")"]},{"cell_type":"markdown","id":"f21aaf6c","metadata":{"id":"f21aaf6c"},"source":["---"]},{"cell_type":"markdown","id":"80bd2219","metadata":{"id":"80bd2219"},"source":["#### (B). *Sistemas de Recomendación:*"]},{"cell_type":"markdown","id":"df7c3ff5","metadata":{"id":"df7c3ff5"},"source":["**Se desea diseñar el sistema de recomendación y utilizarlo para recomendarnos películas.**\n","\n","- **Agregar un usuario a la base de datos con al menos $10$ películas calificadas. Utilice el buscador para no tener que escribir los títulos perfectos.**\n","\n","- **Utilizando gradiente descendente entrenar un filtro colaborativo con un espacio latente de dimensión $10$, $\\lambda = 10$ y *learning rate* $10^{−3}$. Graficar el riesgo regularizado empírico en función del número de iteraciones (al menos $2000$).**\n","\n","- **Crear un *rating* ponderando en partes iguales la salida del filtro colaborativo y la calificación media de las películas.**\n","\n","- **Recomendar las $5$ películas no vistas con más alto *rating* al usuario creado anteriormente.**\n","\n","&ensp; Para desarrollar este apartado del trabajo, se implementa nuevamente una clase llamada $\\texttt{RecommenderSystem}$, cuyo objetivo es definir un sistema de recomendación según lo visto en las clases teóricas de la materia.\n","\n","&ensp; Dentro de los métodos de la clase, se encuentran los siguientes:\n","\n","- `init:` Inicializa la clase y declara los atributos necesarios para almacenar las variables del modelo, entre las que se encuentran: el *dataframe* de las películas, las clasificaciones de los usuarios para cada película, una lista para almacenar el costo en cada iteración del entrenamiento y los parámetros de entrenamiento como $x$ y $\\theta$.\n","\n","- `add_user:` Agrega un usuario a la base de datos pasada como parámetro en la inicialización. Para ello, se utiliza la clase $\\texttt{LanguageModel}$, que mejora el entendimiento de los títulos pasados. Además, en base a los títulos y los puntajes que se le pasan al método, se actualiza el *dataframe* con los *scores* correspondientes a cada película.\n","\n","- `train_collab_filter:` Calcula los parámetros $x \\in \\mathbb{R}^{n_\\text{items} \\times k}$ y $\\theta \\in \\mathbb{R}^{n_\\text{users} \\times k}$ del filtro colaborativo. A fin de realizar esto, se comienza planteando que se quiere minimizar la siguiente expresión:\n","$$\\min_{x, ~\\theta} \\underbrace{\\frac{1}{2} \\sum_{(i, ~j): ~ y_{i, ~j} > 0} \\left( \\theta_j^T \\cdot x_i - y_{i, ~j} \\right)^2 + \\frac{\\lambda}{2} \\left( \\sum_{i = 1}^{n_{\\text{items}}} ||x_i||^2 +  \\sum_{j = 1}^{n_{\\text{users}}} ||\\theta_j||^2 \\right)}_{J(x_i, ~\\theta_j)};$$\n","donde $y_{i, ~j} \\in \\mathbb{R}^{n_{\\text{items}} \\times n_{\\text{users}}}$ corresponde a la clasificación del *dataset* en la fila $i$ columna $j$, $k$ es la dimensión del espacio latente y $\\lambda$ es un hiperparámetro de regularización. Así, se plantean las derivadas parciales de $J(x_i, ~\\theta_j)$, tal que:\n","\\begin{align*}\n","\\begin{cases}\n","  \\frac{\\partial}{\\partial x_i} J(x_i, ~ \\theta_j) &= \\sum_{j: ~ y_{i, ~j} > 0} \\left( \\theta_j^T \\cdot x_i - y_{i, ~j} \\right) \\cdot \\theta_j + \\lambda \\cdot x_i \\\\\n","  \\frac{\\partial}{\\partial \\theta_j} J(x_i, ~ \\theta_j) &= \\sum_{i: ~ y_{i, ~j} > 0} \\left( \\theta_j^T \\cdot x_i - y_{i, ~j} \\right) \\cdot x_i + \\lambda \\cdot \\theta_j\n","\\end{cases}\n","\\end{align*}\n","De esta manera, se obtienen los gradientes de cada una de las variables a determinar, por lo tanto, como se trabaja con el método del gradiente descendente, se tiene que los parámetros de entrenamiento que minimizan la función costo están dados por:\n","\\begin{align*}\n","\\begin{cases}\n","  x_{i, ~ t}  = x_{i, ~ t - 1} - \\alpha \\cdot \\frac{\\partial}{\\partial x_i} J(x_i, ~ \\theta_j) \\\\\n","  \\theta_{j, ~ t}  = \\theta_{j, ~ t - 1} - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_j} J(x_i, ~ \\theta_j)\n","\\end{cases}\n","\\end{align*}\n","Además, el método calcula el costo según la expresión de $J(x_i, ~ \\theta_j)$ en cada iteración y lo almacena en la lista inicializada anteriormente.\n","\n","- `rating_ponderation:` Computa un *rating* considerando la salida del filtro colaborativo y la calificación media de las películas, con una proporción dada por el hiperparámetro $0 \\leq p \\leq 1$. Esto, se realiza a partir de la siguiente expresión:\n","$$\\hat{y}_{i, ~ j} = p \\left(\\theta_j^T \\cdot x_i \\right) + (1 - p) \\cdot \\tilde{y}_i;$$\n","donde $\\tilde{y}_i$ es la calificación promedio del $i$-ésimo item.\n","\n","- `recommend:` A partir de los *ratings* estimados para el usuario sobre las películas disponibles, se recomienda un conjunto de $n$ películas no vistas que presentan los valores de *rating* más altos."]},{"cell_type":"code","execution_count":null,"id":"SjsTiV0H16KF","metadata":{"id":"SjsTiV0H16KF"},"outputs":[],"source":["class RecommenderSystem:\n","    def __init__(self, dataframe):\n","        if \"Name\" not in dataframe.columns:\n","            raise ValueError(\"El DataFrame debe contener una columna 'Name'.\")\n","\n","        self.df = dataframe\n","        self.y = None\n","        self.y_qualified = None\n","        self.x = None\n","        self.theta = None\n","        self.cost = []\n","\n","    def add_user(self, language_model, new_user, movies, scores):\n","        if len(movies) != len(scores):\n","          raise ValueError(\"La cantidad de películas y puntajes debe coincidir.\")\n","\n","        self.df[new_user] = 0\n","        for i in range(len(movies)):\n","          best_title = language_model.searcher(movies[i])\n","          self.df.loc[self.df['Name'] == best_title, new_user] = scores[i]\n","        return self.df.loc[self.df[new_user] != 0, ['Name', new_user]]\n","\n","    def train_collab_filter(self, learning_rate, lambda_, k, n_iter):\n","        self.y = self.df.drop(columns=[\"Name\"]).values.astype(float)\n","        self.y_qualified = (self.y > 0).astype(int)\n","        n_items, n_users = self.y.shape\n","\n","        self.x = np.random.normal(loc=0, scale=0.1, size=(n_items, k))\n","        self.theta = np.random.normal(loc=0, scale=0.1, size=(n_users, k))\n","\n","        for i in range(n_iter):\n","          error = (self.x @ self.theta.T - self.y) * self.y_qualified\n","          x_gradient = error @ self.theta + lambda_ * self.x\n","          theta_gradient = error.T @ self.x + lambda_ * self.theta\n","\n","          self.x -= learning_rate * x_gradient\n","          self.theta -= learning_rate * theta_gradient\n","\n","          cost_aux = 0.5 * np.sum(error**2) + (lambda_ / 2) * (np.sum(self.x**2) + np.sum(self.theta**2))\n","          self.cost.append(cost_aux)\n","\n","    def rating_ponderation(self, p):\n","        sum_scores = np.sum(self.y, axis=1, keepdims=True)\n","        count_scores = np.sum(self.y_qualified, axis=1, keepdims=True)\n","        y_mean_score =  np.divide(sum_scores, count_scores, out=np.zeros_like(sum_scores), where=count_scores!=0)\n","\n","        return p * (self.x @ self.theta.T) + (1 - p) * y_mean_score\n","\n","    def recommend(self, user, n, p):\n","        if user not in self.df.columns:\n","          raise ValueError(\"El usuario no está en la base de datos.\")\n","\n","        idx_user = self.df.columns.get_loc(user) - 1\n","        idx_unseen = np.where(self.y_qualified[:, idx_user] == 0)[0]\n","        ratings_user = self.rating_ponderation(p)[idx_unseen, idx_user]\n","\n","        top_movies = idx_unseen[np.argsort(ratings_user)[::-1][:n]]\n","        return self.df.iloc[top_movies]['Name']"]},{"cell_type":"markdown","id":"5TCtn8qN3Jen","metadata":{"id":"5TCtn8qN3Jen"},"source":["&ensp; Una vez implementada la clase, se la inicializa y se agrega el usuario `Felipe` con puntuaciones de $14$ películas. A continuación, se puede observar una tabla con las películas calificadas."]},{"cell_type":"code","execution_count":null,"id":"VaQhAYkAVowK","metadata":{"id":"VaQhAYkAVowK"},"outputs":[],"source":["RS = RecommenderSystem(DF)\n","\n","user = \"Felipe\"\n","films = [\"rear window\", \"birds\", \"godfather\", \"goodfellas\", \"pulp\", \"stand by\", \"cape\", \"se7en\", \"boogie\", \"casino\", \"pretty\", \"innocence\", \"gilbert\", \"professional\"]\n","scores = [4, 4, 5, 5, 3, 4, 4, 5, 2, 5, 3, 3, 4, 3]\n","RS.add_user(LM, user, films, scores)\n"]},{"cell_type":"markdown","id":"twhEqHAU47Ct","metadata":{"id":"twhEqHAU47Ct"},"source":["&ensp; Posteriormente, se entrena el filtro colaborativo según lo solicitado en la cátedra y se grafica el costo en función del número de iteraciones."]},{"cell_type":"code","execution_count":null,"id":"TK4RdQxecI9F","metadata":{"id":"TK4RdQxecI9F"},"outputs":[],"source":["iterations = 2000\n","RS.train_collab_filter(learning_rate=1e-3, lambda_=10, k=10, n_iter=iterations)"]},{"cell_type":"code","execution_count":null,"id":"Gyf6CRP4dJKn","metadata":{"id":"Gyf6CRP4dJKn"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","plt.plot(np.arange(0, iterations), RS.cost)\n","plt.title(\"$J(x_i, ~ θ_j)$ vs n iteraciones\")\n","plt.xlabel(\"Cantidad de Iteraciones\")\n","plt.ylabel(\"Costo\")\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","id":"qvPoKiIN5VXo","metadata":{"id":"qvPoKiIN5VXo"},"source":["&ensp; En el gráfico se puede observar que, alrededor de la iteración $125$, el costo disminuye considerablemente en relación con su valor inicial. A partir de ese punto, se estabiliza en un valor cercano a los $73000$, manteniéndose prácticamente constante hasta alcanzar las $2000$ iteraciones.\n","\n","&ensp; Por último, se le recomiendan al usuario $5$ películas, según las calificadas anteriormente. De esta manera, se tiene la siguiente tabla:"]},{"cell_type":"code","execution_count":null,"id":"Ph1gb2RsRQrE","metadata":{"id":"Ph1gb2RsRQrE"},"outputs":[],"source":["RS.recommend(user, n=5, p=0.5)"]},{"cell_type":"markdown","id":"451bc45c","metadata":{"id":"451bc45c"},"source":["---\n","---"]},{"cell_type":"markdown","id":"47352633","metadata":{"id":"47352633"},"source":["<div style=\"text-align: center;\">\n","  <h3> Conclusiones\n","</div>"]},{"cell_type":"markdown","id":"TqDs1RKD7dIP","metadata":{"id":"TqDs1RKD7dIP"},"source":["&ensp; El trabajo permitió implementar un sistema completo de recomendación de películas, combinando distintas técnicas vistas en las clases del curso. La utilización de *embeddings* pre-entrenados mediante $\\texttt{GloVe}$ resultó efectiva para capturar similitudes entre títulos de películas, logrando un buscador que soporte títulos incompletos o errores.\n","\n","&ensp; Por otro lado, el sistema de recomendación basado en un filtro colaborativo permitió identificar preferencias del usuario a partir de un conjunto de calificaciones. El entrenamiento mediante *gradient descent* resultó en parámetros útiles que, combinándolos con las calificaciones promedio, permitieron determinar un *rating* para realizar recomendaciones personalizadas al usuario.\n","\n","&ensp; Además, el gráfico del costo en función del número de iteraciones, demostró visualmente cómo a mayor cantidad de iteraciones, el método del gradiente descendiente permite minimizar el costo.\n","\n","&ensp; Como conclusión principal, tanto el modelo de lenguaje como el sistema de recomendación conforman un bloque capaz de interpretar búsquedas y ofrecer sugerencias, destacándose así por la coherencia de las recomendaciones obtenidas y la solidez del desarrollo implementado."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"}},"nbformat":4,"nbformat_minor":5}